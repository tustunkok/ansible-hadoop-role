---
- name: Create spark-env file
  copy:
    src: "{{ __spark_installation_dir }}/conf/spark-env.sh.template"
    dest: "{{ __spark_installation_dir }}/conf/spark-env.sh"
    remote_src: true

- name: Create spark-defaults file
  copy:
    src: "{{ __spark_installation_dir }}/conf/spark-defaults.conf.template"
    dest: "{{ __spark_installation_dir }}/conf/spark-defaults.conf"
    remote_src: true

- name: Set environment variables in spark-env.sh file
  ansible.builtin.lineinfile:
    path: "{{ __spark_installation_dir }}/conf/spark-env.sh"
    regexp: '^export {{ item.conf }}='
    line: 'export {{ item.conf }}={{ item.value }}'
  loop:
    - { 'conf': 'HADOOP_CONF_DIR', 'value': '{{ __hadoop_installation_dir }}/etc/hadoop' }
    - { 'conf': 'SPARK_DIST_CLASSPATH', 'value': '$({{ __hadoop_installation_dir }}/bin/hadoop classpath)' }

- name: Set spark default variables in spark-defaults.conf file
  ansible.builtin.lineinfile:
    path: "{{ __spark_installation_dir }}/conf/spark-defaults.conf"
    regexp: '^{{ item.conf }}'
    line: '{{ item.conf }} {{ item.value }}'
  loop:
    - { 'conf': 'spark.master', 'value': 'yarn' }
    - { 'conf': 'spark.driver.memory', 'value': '512m' }
    - { 'conf': 'spark.executor.memory  ', 'value': '512m' }
    - { 'conf': 'spark.executor.cores  ', 'value': '1' }
    - { 'conf': 'spark.executor.instances  ', 'value': '1' }
    - { 'conf': 'spark.driver.cores', 'value': '1' }
    - { 'conf': 'spark.deploy.mode', 'value': 'cluster' }
